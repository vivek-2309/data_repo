# The main job for dm_project.
resources:
  jobs:
    dm_project_job:
      name: dm_project_job

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      email_notifications:
        on_failure:
          - rajesh.rathod@e-zest.in

      tasks:
        - task_key: notebook_task
          existing_cluster_id: 0603-103701-vhu70phv
          notebook_task:
            notebook_path: ../src/util.ipynb

        # - task_key: notebook_task
        #   existing_cluster_id: 0603-103701-vhu70phv
        #   notebook_task:
        #     notebook_path: ../src/notebook.ipynb
        
        # - task_key: refresh_pipeline
        #   depends_on:
        #     - task_key: notebook_task
        #   pipeline_task:
        #     pipeline_id: ${resources.pipelines.dm_project_pipeline.id}
        
        # - task_key: main_task
        #   depends_on:
        #     - task_key: refresh_pipeline
        #   existing_cluster_id: 0603-103701-vhu70phv
          # python_wheel_task:
          #   package_name: dm_project
          #   entry_point: main
          # libraries:
          #   # By default we just include the .whl file generated for the dm_project package.
          #   # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
          #   # for more information on how to add other libraries.
          #   - whl: ../dist/*.whl

      # job_clusters:
      #   - job_cluster_key: job_cluster
      #     new_cluster:
      #       spark_version: 15.4.x-scala2.12
      #       node_type_id: i3.xlarge
      #       data_security_mode: SINGLE_USER
      #       autoscale:
      #           min_workers: 1
      #           max_workers: 4
